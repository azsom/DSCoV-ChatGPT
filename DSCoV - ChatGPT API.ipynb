{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df4480b-e8bf-4df0-937e-38ad6e9bedce",
   "metadata": {},
   "source": [
    "# The ChatGPT API\n",
    "\n",
    "By the end of this talk, you will be able to:\n",
    "- interact with ChatGPT in a jupyter-notebook/google colab/VSCode\n",
    "- summarize text\n",
    "- perform sentiment analysis\n",
    "- develop a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace39b5-51ed-48bf-9560-02faf4f56766",
   "metadata": {},
   "source": [
    "## Intro\n",
    "- GPT is short for Generative Pre-trained Transformer model\n",
    "    - it provides text outputs in response to text inputs (prompts)\n",
    "    - prompts have four goals:\n",
    "        - ask a question \n",
    "        - provide detailed instructions\n",
    "        - provide some examples of how to successfully complete a task\n",
    "        - provide domain knowledge ChatGPT needs to know to complete a task\n",
    "- ChatGPT is a large language model (LLM) improved by reinforcement learning with human feedback (RLHF)\n",
    "- You can interact with it in two ways:\n",
    "    - web interface: https://chat.openai.com/\n",
    "        - free or \\$20/month for a ChatGPT Plus plan \n",
    "    - API access mostly for developers to build chat-based applications\n",
    "        - token-based, I paid less than \\$0.05 to develop and test code for this talk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f220d-4c4a-447f-8e7d-338a1466b498",
   "metadata": {},
   "source": [
    "## Warning\n",
    "- ChatGPT is a third party software\n",
    "- Everything you ask and the responses you receive are collected and stored by openai\n",
    "- DO NOT share sensitive data and personally identifiable info (PII) with AI tools such as ChatGPT, Bard, Github Copilot, etc.\n",
    "- No Level 2 and 3\n",
    "<center><img src=\"datariskclassification.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c66595-6fdb-4a4f-8ecb-e255390d6f7a",
   "metadata": {},
   "source": [
    "<font color='LIGHTGRAY'>By the end of this talk, you will be able to:</font>\n",
    "- **interact with ChatGPT in a jupyter-notebook/google colab/VSCode**\n",
    "- <font color='LIGHTGRAY'>summarize text</font>\n",
    "- <font color='LIGHTGRAY'>perform sentiment analysis</font>\n",
    "- <font color='LIGHTGRAY'>develop a chatbot</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d78847-0b90-4926-abb1-b0bbc15a7966",
   "metadata": {},
   "source": [
    "## The get_completion() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5b944d-fa31-4e67-bab5-0192fd9bc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d68f7d3-a535-4607-9f05-6a11de472577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompts, roles = ['user'], model = \"gpt-3.5-turbo\", temperature = 0, n = 1, verbose = False):\n",
    "    '''\n",
    "    prompts: str or list\n",
    "        If str, it is a single prompt. If a list, it contains a list of strings in a message history.\n",
    "    roles: list, default is ['user']\n",
    "        A list of roles in a message history, usually the elements are 'user' or 'assistant'.\n",
    "    model: str, default is \"gpt-3.5-turbo\"\n",
    "        The specific model version to be used for generating the response.\n",
    "    temperature: float between 0 and 2, default is 0\n",
    "        The degree of randomness of the model's output.\n",
    "    n: int, default is 1\n",
    "        The number of completions to generate.\n",
    "    verbose: boolean, default is False\n",
    "        If True, the input messages and the full response in JSON format are printed. \n",
    "\n",
    "    Returns: str, list, or JSON object\n",
    "        The model's response. It is a string if n = 1 and verbose == False. \n",
    "        It is a list if n > 1 and verbose == False. It is a JSON object if verbose == True.\n",
    "    \n",
    "    Use the prompts and roles lists to provide message history. \n",
    "    This is useful if chatGPT needs context for a successful response.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "    prompts = ['Tell me a joke.', 'Why did the chicken cross the road?', 'I don’t know, why did the chicken cross the road?']\n",
    "    roles = ['user','assistant','user'] \n",
    "\n",
    "    The response will be the punchline of the joke.\n",
    "    '''\n",
    "\n",
    "    # check inputs and prepare messages\n",
    "    if type(prompts) == str:\n",
    "        messages = [{'role':'user','content':prompts}]\n",
    "    elif type(prompts) == list:\n",
    "        if len(roles) != len(prompts):\n",
    "            raise ValueError('Lengths of roles and prompts are not equal!')\n",
    "        # combine roles and prompts\n",
    "        messages = [{\"role\":roles[i],\"content\":prompts[i]} for i in range(len(roles))] \n",
    "    else:\n",
    "        raise ValueError('prompts is neither a string nor a list!')\n",
    "        \n",
    "    if verbose:\n",
    "        print(messages)\n",
    "\n",
    "    # query ChatGPT\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n = n,\n",
    "        temperature=temperature, \n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # return the full response as a JSON object\n",
    "        return response\n",
    "    else:\n",
    "        if n == 1: \n",
    "            # return the only response as a string\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        else:\n",
    "            # return all responses as a list of strings\n",
    "            return [choice.message[\"content\"] for choice in response.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1ab04e-bb17-40a4-a688-a593b9c81458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a classic one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "# example of a simple prompt, no message history\n",
    "prompt = 'Tell me a joke!'\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab64425-bc7e-46af-a753-8c8507ccb446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get to the other side!\n"
     ]
    }
   ],
   "source": [
    "roles = ['user','assistant','user']\n",
    "prompts = ['Tell me a joke.', 'Why did the chicken cross the road?', 'I don’t know, why did the chicken cross the road?']\n",
    "\n",
    "response = get_completion(prompts,roles)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50acab0-21f6-43d8-b88d-d2f03fe1566e",
   "metadata": {},
   "source": [
    "## Summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea89e5-b6be-4e2c-874b-7626d8bee1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2fad7f8-9a87-43ba-8613-b9e2a6b712e4",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1706f6a-5cf1-42eb-812d-17efa4b0a588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bade20a-4861-4871-8de9-c6438eb896ea",
   "metadata": {},
   "source": [
    "## Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca1ffa-3934-4ba5-a55e-55225e981168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
